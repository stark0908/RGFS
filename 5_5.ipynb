{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch sees\", torch.cuda.device_count(), \"GPUs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee91f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"CPU cores available:\", os.cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6624c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets.utils import download_and_extract_archive\n",
    "from torchvision.datasets.folder import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da86ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223056ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"/home/23dcs505/data/2750\"):\n",
    "    print(\"No dataset found\")\n",
    "fulldata=ImageFolder(root='/home/23dcs505/data/2750', transform=data_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a019bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_len=int((0.8)*len(fulldata))\n",
    "test_len=len(fulldata)-(train_len)\n",
    "\n",
    "train_data_set,test_data_set= random_split(fulldata,[train_len, test_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1075bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_list=[0,1,2,3,4,5,6,7,8,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c4065",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_len=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list=random.sample(all_list,train_class_len)\n",
    "test_list=list(range(0,10))\n",
    "strict_test_list=list(set(all_list) - set(train_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec17bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_list)\n",
    "print(test_list)\n",
    "print(strict_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fe318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ways=5\n",
    "shots=5\n",
    "queries=5\n",
    "strict_ways=ways\n",
    "gpu_num=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea19459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1639b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set.indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71adb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_sorting(dataset, class_list):\n",
    "    targets = dataset.dataset.targets\n",
    "\n",
    "    indices= [i for i in dataset.indices if targets[i] in class_list]\n",
    "    return Subset(dataset.dataset, indices)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9800fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=class_sorting(train_data_set,train_list)\n",
    "test_data=class_sorting(test_data_set,test_list)\n",
    "strict_test_data=class_sorting(test_data_set,strict_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced87a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7951d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9314ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10708bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_patches_tensor(image, patch_size=9, num_patches=124):\n",
    "    masked = image.clone()\n",
    "    _, H, W = masked.shape\n",
    "\n",
    "    for _ in range(num_patches):\n",
    "        top = torch.randint(0, H - patch_size + 1, (1,)).item()\n",
    "        left = torch.randint(0, W - patch_size + 1, (1,)).item()\n",
    "        masked[:, top:top + patch_size, left:left + patch_size] = 0.0 \n",
    "\n",
    "    return masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35037a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class create_dataset(Dataset):\n",
    "    def __init__(self,data,way,shot,query,episode):\n",
    "        super().__init__()\n",
    "        self.data=data\n",
    "        self.way=way\n",
    "        self.shot=shot\n",
    "        self.query=query\n",
    "        self.episode=episode\n",
    "\n",
    "        self.class_to_indices=self._build_class_index()\n",
    "        self.classes=list(self.class_to_indices.keys())\n",
    "    \n",
    "    @staticmethod\n",
    "    def block_mask(img, patch_size=8, mask_ratio=0.1):\n",
    "        C, H, W = img.shape\n",
    "        num_patches_h = H // patch_size\n",
    "        num_patches_w = W // patch_size\n",
    "        total_patches = num_patches_h * num_patches_w\n",
    "        num_mask = int(mask_ratio * total_patches)\n",
    "\n",
    "        # Choose random patch indices to mask\n",
    "        patch_indices = [(i, j) for i in range(num_patches_h) for j in range(num_patches_w)]\n",
    "        masked_indices = random.sample(patch_indices, num_mask)\n",
    "\n",
    "        # Initialize full mask\n",
    "        mask = torch.zeros((1, H, W))\n",
    "\n",
    "        for i, j in masked_indices:\n",
    "            h_start = i * patch_size\n",
    "            w_start = j * patch_size\n",
    "            mask[:, h_start:h_start+patch_size, w_start:w_start+patch_size] = 1.0\n",
    "\n",
    "        masked_img = img.clone() * (1 - mask)\n",
    "        return masked_img, mask    \n",
    "\n",
    "    def _build_class_index(self):\n",
    "        class_index={}\n",
    "\n",
    "        targets=self.data.dataset.targets\n",
    "\n",
    "        labels = [self.data.dataset.targets[i] for i in self.data.indices]\n",
    "        \n",
    "\n",
    "\n",
    "        for indexofsubset, indexoforiginal in enumerate(self.data.indices):\n",
    "            label=targets[indexoforiginal]\n",
    "            if label not in class_index:\n",
    "                class_index[label]=[]\n",
    "            class_index[label].append(indexofsubset)\n",
    "\n",
    "        return class_index\n",
    "        \n",
    "    def __len__(self):\n",
    "            return self.episode\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        selected_class=random.sample(self.classes,self.way)\n",
    "\n",
    "        reconstruct_images, support_images, support_labels=[],[],[]\n",
    "        query_images, query_labels=[],[]\n",
    "        mask=[]\n",
    "\n",
    "\n",
    "        label_map={class_name: i for i, class_name in enumerate(selected_class)}\n",
    "\n",
    "        for class_name in selected_class:\n",
    "            all_indices_for_class=self.class_to_indices[class_name]\n",
    "\n",
    "            selected_index=random.sample(all_indices_for_class,self.shot+self.query)\n",
    "\n",
    "            support_index=selected_index[:self.shot]\n",
    "            query_index=selected_index[self.shot:]\n",
    "\n",
    "            for i in support_index:\n",
    "                image,_=self.data[i]\n",
    "                support_images.append(image)\n",
    "\n",
    "                masked_image,masks = self.block_mask(image)\n",
    "                reconstruct_images.append(masked_image)\n",
    "                mask.append(masks)\n",
    "                support_labels.append(torch.tensor(label_map[class_name]))\n",
    "                \n",
    "            for i in query_index:\n",
    "                image,_=self.data[i]\n",
    "                query_images.append(image)\n",
    "                query_labels.append(torch.tensor(label_map[class_name]))\n",
    "            \n",
    "        return(\n",
    "            torch.stack(reconstruct_images),\n",
    "            torch.stack(mask),\n",
    "            torch.stack(support_images),\n",
    "            torch.stack(support_labels),\n",
    "            torch.stack(query_images),\n",
    "            torch.stack(query_labels)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4451cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prototypes(support_embeddings,support_labels,way):\n",
    "    embedding_dimensions=support_embeddings.size(-1)\n",
    "    prototypes=torch.zeros(way,embedding_dimensions).to(support_embeddings.device)\n",
    "\n",
    "    for c in range(way):\n",
    "        class_mask=(support_labels==c)\n",
    "        class_embeddings=support_embeddings[class_mask]\n",
    "        prototypes[c]=class_embeddings.mean(dim=0)\n",
    "    return prototypes\n",
    "\n",
    "def classify_queries(prototypes,query_embeddings):\n",
    "    n_query=query_embeddings.size(0)\n",
    "    way=prototypes.size(0)\n",
    "\n",
    "    query_exp=query_embeddings.unsqueeze(1).expand(n_query,way,-1)\n",
    "    prototypes_exp=prototypes.unsqueeze(0).expand(n_query,way,-1)\n",
    "\n",
    "    distances=torch.sum((query_exp-prototypes_exp)**2,dim=2)\n",
    "\n",
    "    logits=-distances\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775c60ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "few_dataset=create_dataset(\n",
    "    data=train_data,\n",
    "    way=ways,\n",
    "    shot=shots,\n",
    "    query=queries,\n",
    "    episode=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e9bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_dataloader=DataLoader(\n",
    "    few_dataset,\n",
    "    #batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=8, \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e04701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4604d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class VGGEmbedding(nn.Module):\n",
    "    \n",
    "#     def __init__(self, embedding_dim=256):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         vgg = models.vgg16(pretrained=True)\n",
    "        \n",
    "#         self.features = vgg.features\n",
    "#         self.avgpool = vgg.avgpool\n",
    "        \n",
    "#         in_features = vgg.classifier[0].in_features\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(in_features, 4096),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(4096, 4096),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(4096, embedding_dim)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "# model= VGGEmbedding(embedding_dim=256)\n",
    "\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "\n",
    "# for param in model.features[24:].parameters():\n",
    "#     param.requires_grad = True\n",
    "    \n",
    "\n",
    "# for param in model.classifier.parameters():\n",
    "#     param.requires_grad = True\n",
    "\n",
    "\n",
    "# device = torch.device(f\"cuda:{gpu_num}\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model=model.to(device)\n",
    "\n",
    "# trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "# optimizer = optim.Adam(trainable_params, lr=1e-4)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e1809",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=create_dataset(\n",
    "    data=test_data,\n",
    "    way=ways,\n",
    "    shot=shots,\n",
    "    query=queries,\n",
    "    episode=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532fa90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader=DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=True,\n",
    "    num_workers=8, \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"testing on class :\",test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd745aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_test_dataset=create_dataset(\n",
    "    data=strict_test_data,\n",
    "    way=strict_ways,\n",
    "    shot=shots,\n",
    "    query=queries,\n",
    "    episode=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96358b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_test_dataloader=DataLoader(\n",
    "    strict_test_dataset,\n",
    "    #batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=8, \n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19bd3f",
   "metadata": {},
   "source": [
    "**Stable Protypical Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16ec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Delete all unused objects\n",
    "gc.collect()\n",
    "\n",
    "# Empty PyTorch CUDA cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea490bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dropblock import DropBlock2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62073d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class DropBlock2D(nn.Module):\n",
    "    def __init__(self, drop_prob=0.1, block_size=3):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.drop_prob == 0.:\n",
    "            return x\n",
    "        else:\n",
    "            gamma = self._compute_gamma(x)\n",
    "            mask = (torch.rand(x.shape[0], 1, x.shape[2], x.shape[3], device=x.device) < gamma).float()\n",
    "            mask = self._compute_block_mask(mask)\n",
    "            countM = mask.numel()\n",
    "            count_ones = mask.sum()\n",
    "            return mask * x * (countM / count_ones)\n",
    "\n",
    "    def _compute_block_mask(self, mask):\n",
    "        block_mask = nn.functional.max_pool2d(\n",
    "            input=mask,\n",
    "            kernel_size=(self.block_size, self.block_size),\n",
    "            stride=(1, 1),\n",
    "            padding=self.block_size // 2\n",
    "        )\n",
    "        return 1 - block_mask\n",
    "\n",
    "    def _compute_gamma(self, x):\n",
    "        return self.drop_prob / (self.block_size ** 2)\n",
    "\n",
    "# Encoder based on ResNet-50\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, drop_prob=0.3, block_size=3):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Freeze all parameters initially\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze parameters of the last block (layer4) for fine-tuning\n",
    "        for param in resnet.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        # Create a sequence of layers, inserting DropBlock after layer3 and layer4.\n",
    "        # This is analogous to the original VGG implementation where DropBlock was\n",
    "        # inserted after major feature extraction/downsampling stages.\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "            resnet.layer2,\n",
    "            resnet.layer3,\n",
    "            DropBlock2D(drop_prob=drop_prob, block_size=block_size),\n",
    "            resnet.layer4,\n",
    "            DropBlock2D(drop_prob=drop_prob, block_size=block_size)\n",
    "        )\n",
    "\n",
    "        # The output of ResNet-50's layer4 has 2048 channels.\n",
    "        # The Decoder and embedding head expect 512 channels.\n",
    "        # This bottleneck layer reduces the channel dimension to ensure compatibility.\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(2048, 512, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Ensure the new bottleneck layer is trainable\n",
    "        for param in self.bottleneck.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.bottleneck(x)   # Output shape: [B, 512, 7, 7]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.vgg = models.vgg16(pretrained = True)\n",
    "\n",
    "#         for param in self.vgg.features.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#         self.feature_extractor = nn.Sequential(*list(self.vgg.features.children())) # Use vgg.features\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         x = self.feature_extractor(x) # Output shape: [B, 512, 7, 7]\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65378cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            #formula to calcu;ate dim in convtranspose is used such that always double easier to deal ig\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),  # 7 → 14 (7(in)-1)*2 -2*1(pad) + 3(ker_size) + 1(out_pad)\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),  # 14 → 28\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),   # 28 → 56\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),    # 56 → 112\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),    # 112 → 224\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1),  # Keep output channels 3 (RGB)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)  # Output: [B, 3, 224, 224]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388633e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoencoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.embedding_head = nn.Sequential(\n",
    "        nn.AdaptiveAvgPool2d(1),\n",
    "        nn.Flatten(),             \n",
    "        nn.Linear(512, 256)\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, masked_img, mask=None):\n",
    "        latent = self.encoder(masked_img)\n",
    "        recon = self.decoder(latent)\n",
    "        embedding=self.embedding_head(latent)\n",
    "        return recon,embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954a28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:{gpu_num}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e8ea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "model = MaskedAutoencoder(encoder, decoder).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3907828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e78c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psnr(mse,max_val=1.0):\n",
    "    psnr = 20 * torch.log10(max_val / torch.sqrt(mse))\n",
    "    return psnr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c66374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(img1, img2):\n",
    "    return torch.sqrt(torch.mean((img1 - img2) ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def identity_loss_fn(embedding_a, embedding_b):\n",
    "    \n",
    "    return F.l1_loss(embedding_a, embedding_b)\n",
    "    \n",
    "    #return F.mse_loss(embedding_a, embedding_b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce29b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(recon, target, mask):\n",
    "    mask = mask.float()\n",
    "    if mask.shape[1] == 1:\n",
    "        mask = mask.expand_as(recon)  # Now shape is [B, 3, H, W]\n",
    "    \n",
    "    loss = F.mse_loss(recon * mask, target * mask, reduction='sum')\n",
    "    norm = mask.sum() + 1e-8\n",
    "    return loss / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56774a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images,mask,support_images,_,_,_=few_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc68f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee318bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images[2].permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f421c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "image_loss=nn.L1Loss()\n",
    "triplet_loss_fn = nn.TripletMarginLoss(margin=1.5, p=2)\n",
    "model = model.to(device)\n",
    "epochs=20\n",
    "\n",
    "#From code for SPN\n",
    "recon_weight=5\n",
    "n_times=15\n",
    "alpha=0.01\n",
    "best_accuracy = 0.0\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_queries,total_final_psnr,total_final_recon_loss= 0,0,0,0,0\n",
    "\n",
    "    from tqdm.notebook import tqdm\n",
    "    progress_bar=tqdm(few_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\",leave=False)\n",
    "\n",
    "    total_recon_loss = 0\n",
    "    total_psnr = 0\n",
    "\n",
    "    for episode in progress_bar:\n",
    "        images,mask,support_images, support_labels, query_images, query_labels=episode\n",
    "        images = images.squeeze(0).to(device, non_blocking=True) \n",
    "        mask = mask.squeeze(0).to(device, non_blocking=True)\n",
    "        support_images=(support_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        query_images=(query_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        support_labels=(support_labels.view(-1)).to(device, non_blocking=True)\n",
    "        query_labels=(query_labels.view(-1)).to(device, non_blocking=True)\n",
    "        \n",
    "    \n",
    "        total_combined_loss=0\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # all_triplet_loss=[]\n",
    "        all_ce_losses = []\n",
    "        all_query_logits = []\n",
    "        all_psnr=[]\n",
    "        all_reconstruct_loss=[]\n",
    "\n",
    "        for _ in range(n_times):\n",
    "            _,support_embeddings=model(support_images)\n",
    "            _,query_embeddings=model(query_images)\n",
    "            n_way=torch.unique(support_labels).size(0)\n",
    "            prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "            logits=classify_queries(prototypes,query_embeddings)\n",
    "            ce_loss=loss_fn(logits,query_labels)\n",
    "\n",
    "            all_ce_losses.append(ce_loss)\n",
    "            all_query_logits.append(logits)\n",
    "\n",
    "            # anchor_embeddings = query_embeddings\n",
    "            # positive_embeddings = prototypes[query_labels] \n",
    "\n",
    "            # dists = torch.cdist(anchor_embeddings, prototypes) # Shape: [num_queries, n_way]\n",
    "            # dists[torch.arange(len(query_labels)), query_labels] = float('inf')\n",
    "            \n",
    "            \n",
    "            # hard_negative_indices = torch.argmin(dists, dim=1)\n",
    "            # negative_embeddings = prototypes[hard_negative_indices].detach()\n",
    "\n",
    "            \n",
    "            #triplet_loss = triplet_loss_fn(\n",
    "                #anchor_embeddings, \n",
    "                #positive_embeddings, \n",
    "                #negative_embeddings\n",
    "            #)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            reconstructed_image,_=model(images,mask)    \n",
    "            recon_loss= masked_loss(reconstructed_image, support_images,mask)\n",
    "            img_loss=image_loss(reconstructed_image,support_images)\n",
    "            recon_loss+=img_loss\n",
    "            mse_loss=F.mse_loss(reconstructed_image,support_images)\n",
    "            psnr=compute_psnr(mse_loss, max_val=1.0)\n",
    "            #all_triplet_loss.append(triplet_loss)\n",
    "            all_reconstruct_loss.append(recon_loss)\n",
    "            all_psnr.append(psnr)\n",
    "\n",
    "        #total_triplet_loss = torch.stack(all_triplet_loss).mean()\n",
    "        total_ce_loss= torch.stack(all_ce_losses).mean()\n",
    "        stacked_logits=torch.stack(all_query_logits)\n",
    "        stacked_probs=torch.softmax(stacked_logits,dim=-1)\n",
    "\n",
    "        true_class_probs = stacked_probs[\n",
    "            torch.arange(n_times)[:, None],\n",
    "            torch.arange(len(query_labels)),\n",
    "            query_labels\n",
    "        ]\n",
    "\n",
    "        total_recon_loss=torch.stack(all_reconstruct_loss).mean()\n",
    "        total_psnr=torch.stack(all_psnr).mean()\n",
    "\n",
    "        variance_loss=torch.std(true_class_probs,dim=0).sum()\n",
    "        total_combined_loss=(recon_weight * total_recon_loss)+(total_ce_loss)+(alpha*variance_loss)\n",
    "\n",
    "        total_combined_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_final_psnr+=total_psnr.item()\n",
    "        total_final_recon_loss+=total_recon_loss.item()\n",
    "        total_loss+=total_combined_loss.item()\n",
    "        mean_logits=stacked_logits.mean(dim=0)\n",
    "        preds=torch.argmax(mean_logits,dim=1)\n",
    "        total_correct+=(preds==query_labels).sum().item()\n",
    "        total_queries+=query_labels.size(0)\n",
    "\n",
    "        avg_acc_till=(total_correct/total_queries)*100\n",
    "        progress_bar.set_postfix(Phase=\"Training\",Loss=f\"{total_combined_loss.item():4f}\",Acc=f\"{avg_acc_till}\",PSNR=f\"{total_psnr.item():4f}&\",Ce_Loss=f\"{total_ce_loss.item():4f}\",Recon_Loss=f\"{total_recon_loss.item():4f}\")\n",
    "\n",
    "    \n",
    "    avg_recon_loss = total_final_recon_loss / len(few_dataloader)\n",
    "    avg_psnr = total_final_psnr / len(few_dataloader)\n",
    "    avg_loss=total_loss/len(few_dataloader)\n",
    "    accuracy=(total_correct/total_queries)*100\n",
    "    print(\"Training \",\"Epoch:\",epoch+1,\"-------------\",\"Loss=\",avg_loss,\"Acccuracy=\",accuracy,\"Recon Loss:\",avg_recon_loss, \"PSNR:\",avg_psnr)\n",
    "\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_queries,total_final_psnr,total_final_recon_loss= 0,0,0,0,0\n",
    "    from tqdm.notebook import tqdm\n",
    "    progress_bar=tqdm(test_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\",leave=False)\n",
    "\n",
    "    total_recon_loss = 0\n",
    "    total_psnr = 0\n",
    "    with torch.no_grad():\n",
    "        for episode in progress_bar:\n",
    "            images,mask,support_images, support_labels, query_images, query_labels=episode\n",
    "            images = images.squeeze(0).to(device, non_blocking=True) \n",
    "            mask = mask.squeeze(0).to(device, non_blocking=True)\n",
    "            support_images=(support_images.squeeze(0)).to(device, non_blocking=True)\n",
    "            query_images=(query_images.squeeze(0)).to(device, non_blocking=True)\n",
    "            support_labels=(support_labels.view(-1)).to(device, non_blocking=True)\n",
    "            query_labels=(query_labels.view(-1)).to(device, non_blocking=True)\n",
    "            \n",
    "\n",
    "            total_combined_loss=0\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            all_ce_losses = []\n",
    "            all_query_logits = []\n",
    "            all_psnr=[]\n",
    "            all_reconstruct_loss=[]\n",
    "            model.train()\n",
    "            for _ in range(n_times):\n",
    "                _,support_embeddings=model(support_images)\n",
    "                _,query_embeddings=model(query_images)\n",
    "                n_way=torch.unique(support_labels).size(0)\n",
    "                prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "                logits=classify_queries(prototypes,query_embeddings)\n",
    "                ce_loss=loss_fn(logits,query_labels)\n",
    "\n",
    "                all_ce_losses.append(ce_loss)\n",
    "                all_query_logits.append(logits)\n",
    "\n",
    "                reconstructed_image,_=model(images,mask)    \n",
    "                recon_loss= masked_loss(reconstructed_image, support_images,mask)\n",
    "                mse_loss=F.mse_loss(reconstructed_image,support_images)\n",
    "                psnr=compute_psnr(mse_loss, max_val=1.0)\n",
    "\n",
    "                all_reconstruct_loss.append(recon_loss)\n",
    "                all_psnr.append(psnr)\n",
    "            model.eval()\n",
    "            \n",
    "            total_ce_loss= torch.stack(all_ce_losses).mean()\n",
    "            stacked_logits=torch.stack(all_query_logits)\n",
    "            stacked_probs=torch.softmax(stacked_logits,dim=-1)\n",
    "\n",
    "            true_class_probs = stacked_probs[\n",
    "                torch.arange(n_times)[:, None],\n",
    "                torch.arange(len(query_labels)),\n",
    "                query_labels\n",
    "            ]\n",
    "\n",
    "            total_recon_loss=torch.stack(all_reconstruct_loss).mean()\n",
    "            total_psnr=torch.stack(all_psnr).mean()\n",
    "\n",
    "            variance_loss=torch.std(true_class_probs,dim=0).sum()\n",
    "            total_combined_loss=(recon_weight * total_recon_loss)+(total_ce_loss)+(alpha*variance_loss)\n",
    "\n",
    "            #total_combined_loss.backward()\n",
    "            #optimizer.step()\n",
    "\n",
    "            total_final_psnr+=total_psnr.item()\n",
    "            total_final_recon_loss+=total_recon_loss.item()\n",
    "            total_loss+=total_combined_loss.item()\n",
    "            mean_logits=stacked_logits.mean(dim=0)\n",
    "            preds=torch.argmax(mean_logits,dim=1)\n",
    "            total_correct+=(preds==query_labels).sum().item()\n",
    "            total_queries+=query_labels.size(0)\n",
    "\n",
    "            avg_acc_till=(total_correct/total_queries)*100\n",
    "            progress_bar.set_postfix(Phase=\"Testing\",Loss=f\"{total_combined_loss.item():4f}\",Acc=f\"{avg_acc_till}\",PSNR=f\"{psnr}&\")\n",
    "\n",
    "\n",
    "        avg_recon_loss = total_final_recon_loss / len(few_dataloader)\n",
    "        avg_psnr = total_final_psnr / len(few_dataloader)\n",
    "        avg_loss=total_loss/len(few_dataloader)\n",
    "        accuracy=(total_correct/total_queries)*100\n",
    "        print(\"Testing \",\"Epoch:\",epoch+1,\"-------------\",\"Loss=\",avg_loss,\"Acccuracy=\",accuracy,\"Recon Loss:\",avg_recon_loss, \"PSNR:\",avg_psnr)\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), \"/home/23dcs505/model_recon/5w5s_resnet.pth\")\n",
    "    \n",
    "    # # Optional: Save the latest model too\n",
    "    # torch.save(model.state_dict(), \"/home/23dcs505/model_recon/latest_model_w_5.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030a94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a64ea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'model_ce_variance_spn_reconstruction.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65030730",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a20241",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_loss, total_correct, total_queries,total_final_psnr,total_final_recon_loss= 0,0,0,0,0\n",
    "from tqdm.notebook import tqdm\n",
    "progress_bar=tqdm(test_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\",leave=False)\n",
    "\n",
    "total_recon_loss = 0\n",
    "total_psnr = 0\n",
    "with torch.no_grad():\n",
    "    for episode in progress_bar:\n",
    "        images,mask,support_images, support_labels, query_images, query_labels=episode\n",
    "        images = images.squeeze(0).to(device, non_blocking=True) \n",
    "        mask = mask.squeeze(0).to(device, non_blocking=True)\n",
    "        support_images=(support_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        query_images=(query_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        support_labels=(support_labels.view(-1)).to(device, non_blocking=True)\n",
    "        query_labels=(query_labels.view(-1)).to(device, non_blocking=True)\n",
    "        \n",
    "\n",
    "        total_combined_loss=0\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        all_ce_losses = []\n",
    "        all_query_logits = []\n",
    "        all_psnr=[]\n",
    "        all_reconstruct_loss=[]\n",
    "        model.train()\n",
    "        for _ in range(n_times):\n",
    "            _,support_embeddings=model(support_images)\n",
    "            _,query_embeddings=model(query_images)\n",
    "            n_way=torch.unique(support_labels).size(0)\n",
    "            prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "            logits=classify_queries(prototypes,query_embeddings)\n",
    "            ce_loss=loss_fn(logits,query_labels)\n",
    "\n",
    "            all_ce_losses.append(ce_loss)\n",
    "            all_query_logits.append(logits)\n",
    "\n",
    "            reconstructed_image,_=model(images,mask)    \n",
    "            recon_loss= masked_loss(reconstructed_image, support_images,mask)\n",
    "            mse_loss=F.mse_loss(reconstructed_image,support_images)\n",
    "            psnr=compute_psnr(mse_loss, max_val=1.0)\n",
    "\n",
    "            all_reconstruct_loss.append(recon_loss)\n",
    "            all_psnr.append(psnr)\n",
    "        model.eval()\n",
    "        \n",
    "        total_ce_loss= torch.stack(all_ce_losses).mean()\n",
    "        stacked_logits=torch.stack(all_query_logits)\n",
    "        stacked_probs=torch.softmax(stacked_logits,dim=-1)\n",
    "\n",
    "        true_class_probs = stacked_probs[\n",
    "            torch.arange(n_times)[:, None],\n",
    "            torch.arange(len(query_labels)),\n",
    "            query_labels\n",
    "        ]\n",
    "\n",
    "        total_recon_loss=torch.stack(all_reconstruct_loss).mean()\n",
    "        total_psnr=torch.stack(all_psnr).mean()\n",
    "\n",
    "        variance_loss=torch.std(true_class_probs,dim=0).sum()\n",
    "        total_combined_loss=(recon_weight * total_recon_loss)+(total_ce_loss)+(alpha*variance_loss)\n",
    "\n",
    "        #total_combined_loss.backward()\n",
    "        #optimizer.step()\n",
    "\n",
    "        total_final_psnr+=total_psnr.item()\n",
    "        total_final_recon_loss+=total_recon_loss.item()\n",
    "        total_loss+=total_combined_loss.item()\n",
    "        mean_logits=stacked_logits.mean(dim=0)\n",
    "        preds=torch.argmax(mean_logits,dim=1)\n",
    "        total_correct+=(preds==query_labels).sum().item()\n",
    "        total_queries+=query_labels.size(0)\n",
    "\n",
    "        avg_acc_till=(total_correct/total_queries)*100\n",
    "        progress_bar.set_postfix(Loss=f\"{total_combined_loss.item():4f}\",Acc=f\"{avg_acc_till}\",PSNR=f\"{psnr}&\")\n",
    "\n",
    "\n",
    "    avg_recon_loss = total_final_recon_loss / len(few_dataloader)\n",
    "    avg_psnr = total_final_psnr / len(few_dataloader)\n",
    "    avg_loss=total_loss/len(few_dataloader)\n",
    "    accuracy=(total_correct/total_queries)*100\n",
    "    print(\"Testing On 10 Classes(Seen + Unseen)\",\"Loss=\",avg_loss,\"Acccuracy=\",accuracy,\"Recon Loss:\",avg_recon_loss, \"PSNR:\",avg_psnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efd799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_loss, total_correct, total_queries,total_final_psnr,total_final_recon_loss= 0,0,0,0,0\n",
    "from tqdm.notebook import tqdm\n",
    "progress_bar=tqdm(strict_test_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\",leave=False)\n",
    "\n",
    "total_recon_loss = 0\n",
    "total_psnr = 0\n",
    "with torch.no_grad():\n",
    "    for episode in progress_bar:\n",
    "        images,mask,support_images, support_labels, query_images, query_labels=episode\n",
    "        images = images.squeeze(0).to(device, non_blocking=True) \n",
    "        mask = mask.squeeze(0).to(device, non_blocking=True)\n",
    "        support_images=(support_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        query_images=(query_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        support_labels=(support_labels.view(-1)).to(device, non_blocking=True)\n",
    "        query_labels=(query_labels.view(-1)).to(device, non_blocking=True)\n",
    "        \n",
    "\n",
    "        total_combined_loss=0\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        all_ce_losses = []\n",
    "        all_query_logits = []\n",
    "        all_psnr=[]\n",
    "        all_reconstruct_loss=[]\n",
    "        model.train()\n",
    "        for _ in range(n_times):\n",
    "            _,support_embeddings=model(support_images)\n",
    "            _,query_embeddings=model(query_images)\n",
    "            n_way=torch.unique(support_labels).size(0)\n",
    "            prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "            logits=classify_queries(prototypes,query_embeddings)\n",
    "            ce_loss=loss_fn(logits,query_labels)\n",
    "\n",
    "            all_ce_losses.append(ce_loss)\n",
    "            all_query_logits.append(logits)\n",
    "\n",
    "            reconstructed_image,_=model(images,mask)    \n",
    "            recon_loss= masked_loss(reconstructed_image, support_images,mask)\n",
    "            mse_loss=F.mse_loss(reconstructed_image,support_images)\n",
    "            psnr=compute_psnr(mse_loss, max_val=1.0)\n",
    "\n",
    "            all_reconstruct_loss.append(recon_loss)\n",
    "            all_psnr.append(psnr)\n",
    "        model.eval()\n",
    "        \n",
    "        total_ce_loss= torch.stack(all_ce_losses).mean()\n",
    "        stacked_logits=torch.stack(all_query_logits)\n",
    "        stacked_probs=torch.softmax(stacked_logits,dim=-1)\n",
    "\n",
    "        true_class_probs = stacked_probs[\n",
    "            torch.arange(n_times)[:, None],\n",
    "            torch.arange(len(query_labels)),\n",
    "            query_labels\n",
    "        ]\n",
    "\n",
    "        total_recon_loss=torch.stack(all_reconstruct_loss).mean()\n",
    "        total_psnr=torch.stack(all_psnr).mean()\n",
    "\n",
    "        variance_loss=torch.std(true_class_probs,dim=0).sum()\n",
    "        total_combined_loss=(recon_weight * total_recon_loss)+(total_ce_loss)+(alpha*variance_loss)\n",
    "\n",
    "        #total_combined_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_final_psnr+=total_psnr.item()\n",
    "        total_final_recon_loss+=total_recon_loss.item()\n",
    "        total_loss+=total_combined_loss.item()\n",
    "        mean_logits=stacked_logits.mean(dim=0)\n",
    "        preds=torch.argmax(mean_logits,dim=1)\n",
    "        total_correct+=(preds==query_labels).sum().item()\n",
    "        total_queries+=query_labels.size(0)\n",
    "\n",
    "        avg_acc_till=(total_correct/total_queries)*100\n",
    "        progress_bar.set_postfix(Loss=f\"{total_combined_loss.item():4f}\",Acc=f\"{avg_acc_till}\",PSNR=f\"{psnr}&\")\n",
    "\n",
    "\n",
    "    avg_recon_loss = total_final_recon_loss / len(few_dataloader)\n",
    "    avg_psnr = total_final_psnr / len(few_dataloader)\n",
    "    avg_loss=total_loss/len(few_dataloader)\n",
    "    accuracy=(total_correct/total_queries)*100\n",
    "    print(\"Testing On 5 Classes(Unseen)\",\"Loss=\",avg_loss,\"Acccuracy=\",accuracy,\"Recon Loss:\",avg_recon_loss, \"PSNR:\",avg_psnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825dcfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/home/23dcs505/model_recon/5w5s_resnet.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71007b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35abcbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_loss, total_correct, total_queries,total_final_psnr,total_final_recon_loss= 0,0,0,0,0\n",
    "from tqdm.notebook import tqdm\n",
    "progress_bar=tqdm(test_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\",leave=False)\n",
    "\n",
    "total_recon_loss = 0\n",
    "total_psnr = 0\n",
    "with torch.no_grad():\n",
    "    for episode in progress_bar:\n",
    "        images,mask,support_images, support_labels, query_images, query_labels=episode\n",
    "        images = images.squeeze(0).to(device, non_blocking=True) \n",
    "        mask = mask.squeeze(0).to(device, non_blocking=True)\n",
    "        support_images=(support_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        query_images=(query_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        support_labels=(support_labels.view(-1)).to(device, non_blocking=True)\n",
    "        query_labels=(query_labels.view(-1)).to(device, non_blocking=True)\n",
    "        \n",
    "\n",
    "        total_combined_loss=0\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        all_ce_losses = []\n",
    "        all_query_logits = []\n",
    "        all_psnr=[]\n",
    "        all_reconstruct_loss=[]\n",
    "        model.train()\n",
    "        for _ in range(n_times):\n",
    "            _,support_embeddings=model(support_images)\n",
    "            _,query_embeddings=model(query_images)\n",
    "            n_way=torch.unique(support_labels).size(0)\n",
    "            prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "            logits=classify_queries(prototypes,query_embeddings)\n",
    "            ce_loss=loss_fn(logits,query_labels)\n",
    "\n",
    "            all_ce_losses.append(ce_loss)\n",
    "            all_query_logits.append(logits)\n",
    "\n",
    "            reconstructed_image,_=model(images,mask)    \n",
    "            recon_loss= masked_loss(reconstructed_image, support_images,mask)\n",
    "            mse_loss=F.mse_loss(reconstructed_image,support_images)\n",
    "            psnr=compute_psnr(mse_loss, max_val=1.0)\n",
    "\n",
    "            all_reconstruct_loss.append(recon_loss)\n",
    "            all_psnr.append(psnr)\n",
    "        model.eval()\n",
    "        \n",
    "        total_ce_loss= torch.stack(all_ce_losses).mean()\n",
    "        stacked_logits=torch.stack(all_query_logits)\n",
    "        stacked_probs=torch.softmax(stacked_logits,dim=-1)\n",
    "\n",
    "        true_class_probs = stacked_probs[\n",
    "            torch.arange(n_times)[:, None],\n",
    "            torch.arange(len(query_labels)),\n",
    "            query_labels\n",
    "        ]\n",
    "\n",
    "        total_recon_loss=torch.stack(all_reconstruct_loss).mean()\n",
    "        total_psnr=torch.stack(all_psnr).mean()\n",
    "\n",
    "        variance_loss=torch.std(true_class_probs,dim=0).sum()\n",
    "        total_combined_loss=(recon_weight * total_recon_loss)+(total_ce_loss)+(alpha*variance_loss)\n",
    "\n",
    "        #total_combined_loss.backward()\n",
    "        #optimizer.step()\n",
    "\n",
    "        total_final_psnr+=total_psnr.item()\n",
    "        total_final_recon_loss+=total_recon_loss.item()\n",
    "        total_loss+=total_combined_loss.item()\n",
    "        mean_logits=stacked_logits.mean(dim=0)\n",
    "        preds=torch.argmax(mean_logits,dim=1)\n",
    "        total_correct+=(preds==query_labels).sum().item()\n",
    "        total_queries+=query_labels.size(0)\n",
    "\n",
    "        avg_acc_till=(total_correct/total_queries)*100\n",
    "        progress_bar.set_postfix(Loss=f\"{total_combined_loss.item():4f}\",Acc=f\"{avg_acc_till}\",PSNR=f\"{psnr}&\")\n",
    "\n",
    "\n",
    "    avg_recon_loss = total_final_recon_loss / len(few_dataloader)\n",
    "    avg_psnr = total_final_psnr / len(few_dataloader)\n",
    "    avg_loss=total_loss/len(few_dataloader)\n",
    "    accuracy=(total_correct/total_queries)*100\n",
    "    print(\"Testing On 10 Classes(Seen + Unseen)\",\"Loss=\",avg_loss,\"Acccuracy=\",accuracy,\"Recon Loss:\",avg_recon_loss, \"PSNR:\",avg_psnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbedf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy = -(torch.softmax(mean_logits, dim=1) * torch.log_softmax(mean_logits, dim=1)).sum(dim=1).mean()\n",
    "# print(\"Mean Predictive Entropy =\", entropy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90766f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_loss, total_correct, total_queries,total_final_psnr,total_final_recon_loss= 0,0,0,0,0\n",
    "from tqdm.notebook import tqdm\n",
    "progress_bar=tqdm(strict_test_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\",leave=False)\n",
    "\n",
    "total_recon_loss = 0\n",
    "total_psnr = 0\n",
    "with torch.no_grad():\n",
    "    for episode in progress_bar:\n",
    "        images,mask,support_images, support_labels, query_images, query_labels=episode\n",
    "        images = images.squeeze(0).to(device, non_blocking=True) \n",
    "        mask = mask.squeeze(0).to(device, non_blocking=True)\n",
    "        support_images=(support_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        query_images=(query_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        support_labels=(support_labels.view(-1)).to(device, non_blocking=True)\n",
    "        query_labels=(query_labels.view(-1)).to(device, non_blocking=True)\n",
    "        \n",
    "\n",
    "        total_combined_loss=0\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        all_ce_losses = []\n",
    "        all_query_logits = []\n",
    "        all_psnr=[]\n",
    "        all_reconstruct_loss=[]\n",
    "        model.train()\n",
    "        for _ in range(n_times):\n",
    "            _,support_embeddings=model(support_images)\n",
    "            _,query_embeddings=model(query_images)\n",
    "            n_way=torch.unique(support_labels).size(0)\n",
    "            prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "            logits=classify_queries(prototypes,query_embeddings)\n",
    "            ce_loss=loss_fn(logits,query_labels)\n",
    "\n",
    "            all_ce_losses.append(ce_loss)\n",
    "            all_query_logits.append(logits)\n",
    "\n",
    "            reconstructed_image,_=model(images,mask)    \n",
    "            recon_loss= masked_loss(reconstructed_image, support_images,mask)\n",
    "            mse_loss=F.mse_loss(reconstructed_image,support_images)\n",
    "            psnr=compute_psnr(mse_loss, max_val=1.0)\n",
    "\n",
    "            all_reconstruct_loss.append(recon_loss)\n",
    "            all_psnr.append(psnr)\n",
    "        model.eval()\n",
    "        \n",
    "        total_ce_loss= torch.stack(all_ce_losses).mean()\n",
    "        stacked_logits=torch.stack(all_query_logits)\n",
    "        stacked_probs=torch.softmax(stacked_logits,dim=-1)\n",
    "\n",
    "        true_class_probs = stacked_probs[\n",
    "            torch.arange(n_times)[:, None],\n",
    "            torch.arange(len(query_labels)),\n",
    "            query_labels\n",
    "        ]\n",
    "\n",
    "        total_recon_loss=torch.stack(all_reconstruct_loss).mean()\n",
    "        total_psnr=torch.stack(all_psnr).mean()\n",
    "\n",
    "        variance_loss=torch.std(true_class_probs,dim=0).sum()\n",
    "        total_combined_loss=(recon_weight * total_recon_loss)+(total_ce_loss)+(alpha*variance_loss)\n",
    "\n",
    "        #total_combined_loss.backward()\n",
    "        #optimizer.step()\n",
    "\n",
    "        total_final_psnr+=total_psnr.item()\n",
    "        total_final_recon_loss+=total_recon_loss.item()\n",
    "        total_loss+=total_combined_loss.item()\n",
    "        mean_logits=stacked_logits.mean(dim=0)\n",
    "        preds=torch.argmax(mean_logits,dim=1)\n",
    "        total_correct+=(preds==query_labels).sum().item()\n",
    "        total_queries+=query_labels.size(0)\n",
    "\n",
    "        avg_acc_till=(total_correct/total_queries)*100\n",
    "        progress_bar.set_postfix(Loss=f\"{total_combined_loss.item():4f}\",Acc=f\"{avg_acc_till}\",PSNR=f\"{psnr}&\")\n",
    "\n",
    "\n",
    "    avg_recon_loss = total_final_recon_loss / len(few_dataloader)\n",
    "    avg_psnr = total_final_psnr / len(few_dataloader)\n",
    "    avg_loss=total_loss/len(few_dataloader)\n",
    "    accuracy=(total_correct/total_queries)*100\n",
    "    print(\"Testing On 5 Classes(Unseen)\",\"Loss=\",avg_loss,\"Acccuracy=\",accuracy,\"Recon Loss:\",avg_recon_loss, \"PSNR:\",avg_psnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b12b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_images(masked, recon, original):\n",
    "    masked = masked.cpu().permute(1, 2, 0).numpy()\n",
    "    recon = recon.cpu().permute(1, 2, 0).numpy()\n",
    "    original = original.cpu().permute(1, 2, 0).numpy()\n",
    "    \n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    axs[0].imshow(masked); axs[0].set_title('Masked Input'); axs[0].axis('off')\n",
    "    axs[1].imshow(recon); axs[1].set_title('Reconstruction'); axs[1].axis('off')\n",
    "    axs[2].imshow(original); axs[2].set_title('Original'); axs[2].axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54bc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_psnr(rmse, max_val=1.0):\n",
    "    psnr = 20 * torch.log10(max_val / rmse)\n",
    "    return psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4450abc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "r=random.randint(1,5000)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    masked_img,mask,img,_,_,_ = few_dataset[10]  # Single sample\n",
    "    masked_img = masked_img.to(device)  # [1, 3, 224, 224]\n",
    "    mask = mask.to(device)              # [1, 1, 224, 224]\n",
    "    img = img.to(device)\n",
    "\n",
    "    recon,_ = model(masked_img, mask)                  # [1, 3, 224, 224]\n",
    "    show_images(masked_img[0], recon[0], img[0])\n",
    "    reconstruction_loss_fn = nn.L1Loss()\n",
    "    recon_loss = reconstruction_loss_fn(recon[0],img[0])\n",
    "    psnr=compute_psnr(recon_loss, max_val=1.0)\n",
    "    print(psnr.item())       # Pass individual tensors to visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af531272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Define the same mean and std you used for normalization\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "def unnormalize(tensor, mean, std):\n",
    "    \"\"\"Reverses the normalization on a tensor.\"\"\"\n",
    "    # Clone the tensor to avoid modifying it in-place\n",
    "    tensor = tensor.clone()\n",
    "    # The un-normalization formula is: pixel = (pixel * std) + mean\n",
    "    # We need to reshape mean and std to broadcast correctly\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "# --- In your visualization code ---\n",
    "def show_images(masked_input, reconstruction, original):\n",
    "    # Make sure tensors are on the CPU\n",
    "    masked_input = masked_input.cpu()\n",
    "    reconstruction = reconstruction.cpu()\n",
    "    original = original.cpu()\n",
    "\n",
    "    # Un-normalize all images\n",
    "    masked_input = unnormalize(masked_input, mean, std)\n",
    "    reconstruction = unnormalize(reconstruction, mean, std)\n",
    "    original = unnormalize(original, mean, std)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Clip values to [0, 1] just in case of small floating point errors\n",
    "    axes[0].imshow(masked_input.permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[0].set_title(\"Masked Input\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(reconstruction.permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[1].set_title(\"Reconstruction\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    axes[2].imshow(original.permute(1, 2, 0).clamp(0, 1))\n",
    "    axes[2].set_title(\"Original\")\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Now call this function with your tensors\n",
    "# show_images(masked_img[0], recon[0], img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80b2716",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=random.randint(1,24)\n",
    "show_images(masked_img[n], recon[n], img[n])\n",
    "reconstruction_loss_fn = nn.L1Loss()\n",
    "recon_loss = reconstruction_loss_fn(recon[n],img[n])\n",
    "psnr=compute_psnr(recon_loss, max_val=1.0)\n",
    "print(psnr.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing........... Started.......................\")\n",
    "model.eval()\n",
    "total_loss, total_correct, total_queries= 0,0,0\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "progress_bar=tqdm(test_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\",leave=False)\n",
    "\n",
    "total_recon_loss = 0\n",
    "total_psnr = 0\n",
    "with torch.no_grad():\n",
    "    for episode in progress_bar:\n",
    "        images,mask,support_images, support_labels, query_images, query_labels=episode\n",
    "        images = images.squeeze(0).to(device, non_blocking=True) \n",
    "        mask = mask.squeeze(0).to(device, non_blocking=True)\n",
    "        support_images=(support_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        query_images=(query_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        support_labels=(support_labels.view(-1)).to(device, non_blocking=True)\n",
    "        query_labels=(query_labels.view(-1)).to(device, non_blocking=True)\n",
    "        \n",
    "        all_psnr=[]\n",
    "        all_reconstruct_loss=[]\n",
    "        total_combined_loss=0\n",
    "\n",
    "        _,support_embeddings=model(support_images)\n",
    "        _,query_embeddings=model(query_images)\n",
    "        n_way=torch.unique(support_labels).size(0)\n",
    "        prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "        logits=classify_queries(prototypes,query_embeddings)\n",
    "        ce_loss=loss_fn(logits,query_labels)\n",
    "        probs=torch.softmax(logits,dim=-1)\n",
    "        true_class_probs = probs[\n",
    "            torch.arange(len(query_labels)),\n",
    "            query_labels\n",
    "        ]\n",
    "\n",
    "        #optimizer.zero_grad(set_to_none=True)\n",
    "        for _ in range(n_times):\n",
    "            exit\n",
    "\n",
    "        reconstructed_image,_=model(images,mask)    \n",
    "        recon_loss= masked_loss(reconstructed_image, support_images,mask)\n",
    "        mse_loss=F.mse_loss(reconstructed_image,support_images)\n",
    "        psnr=compute_psnr(mse_loss, max_val=1.0)\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_psnr += psnr\n",
    "\n",
    "        #variance_loss=torch.std(true_class_probs,dim=0).sum()\n",
    "        total_combined_loss=(recon_weight * recon_loss)+(ce_loss)+(alpha*variance_loss)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        #total_combined_loss.backward()\n",
    "        #optimizer.step()\n",
    "\n",
    "\n",
    "        total_loss+=total_combined_loss.item()\n",
    "        # mean_logits=stacked_logits.mean(dim=0)\n",
    "        preds=torch.argmax(logits,dim=1)\n",
    "        total_correct+=(preds==query_labels).sum().item()\n",
    "        total_queries+=query_labels.size(0)\n",
    "\n",
    "        avg_acc_till=(total_correct/total_queries)*100\n",
    "        progress_bar.set_postfix(Loss=f\"{total_combined_loss.item():4f}\",Acc=f\"{avg_acc_till}\",PSNR=f\"{psnr}&\")\n",
    "\n",
    "\n",
    "    avg_recon_loss = total_recon_loss / len(few_dataloader)\n",
    "    avg_psnr = total_psnr / len(few_dataloader)\n",
    "    avg_loss=total_loss/len(few_dataloader)\n",
    "    accuracy=(total_correct/total_queries)*100\n",
    "    accuracy_spn_0=(total_correct/total_queries)*100\n",
    "    print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of Stable Prototypical Network on all\", len(test_list),\"Class =\",accuracy_spn_0,\"%\",\"PSNR=\",avg_psnr)\n",
    "\n",
    "\n",
    "    entropy = -(torch.softmax(logits, dim=1) * torch.log_softmax(logits, dim=1)).sum(dim=1).mean()\n",
    "    print(\"Mean Predictive Entropy =\", entropy.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025823f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing........... Started.......................\")\n",
    "model.eval()\n",
    "total_loss, total_correct, total_queries= 0,0,0\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "progress_bar=tqdm(strict_test_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\",leave=False)\n",
    "\n",
    "total_recon_loss = 0\n",
    "total_psnr = 0\n",
    "with torch.no_grad():\n",
    "    for episode in progress_bar:\n",
    "        images,mask,support_images, support_labels, query_images, query_labels=episode\n",
    "        images = images.squeeze(0).to(device, non_blocking=True) \n",
    "        mask = mask.squeeze(0).to(device, non_blocking=True)\n",
    "        support_images=(support_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        query_images=(query_images.squeeze(0)).to(device, non_blocking=True)\n",
    "        support_labels=(support_labels.view(-1)).to(device, non_blocking=True)\n",
    "        query_labels=(query_labels.view(-1)).to(device, non_blocking=True)\n",
    "        \n",
    "        all_psnr=[]\n",
    "        all_reconstruct_loss=[]\n",
    "        total_combined_loss=0\n",
    "\n",
    "        _,support_embeddings=model(support_images)\n",
    "        _,query_embeddings=model(query_images)\n",
    "        n_way=torch.unique(support_labels).size(0)\n",
    "        prototypes=compute_prototypes(support_embeddings,support_labels,n_way)\n",
    "        logits=classify_queries(prototypes,query_embeddings)\n",
    "        ce_loss=loss_fn(logits,query_labels)\n",
    "        probs=torch.softmax(logits,dim=-1)\n",
    "        true_class_probs = probs[\n",
    "            torch.arange(len(query_labels)),\n",
    "            query_labels\n",
    "        ]\n",
    "\n",
    "        #optimizer.zero_grad(set_to_none=True)\n",
    "        for _ in range(n_times):\n",
    "            exit\n",
    "\n",
    "        reconstructed_image,_=model(images,mask)    \n",
    "        recon_loss= masked_loss(reconstructed_image, support_images,mask)\n",
    "        mse_loss=F.mse_loss(reconstructed_image,support_images)\n",
    "        psnr=compute_psnr(mse_loss, max_val=1.0)\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_psnr += psnr\n",
    "\n",
    "        #variance_loss=torch.std(true_class_probs,dim=0).sum()\n",
    "        total_combined_loss=(recon_weight * recon_loss)+(ce_loss)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        #total_combined_loss.backward()\n",
    "        #optimizer.step()\n",
    "\n",
    "\n",
    "        total_loss+=total_combined_loss.item()\n",
    "        # mean_logits=stacked_logits.mean(dim=0)\n",
    "        preds=torch.argmax(logits,dim=1)\n",
    "        total_correct+=(preds==query_labels).sum().item()\n",
    "        total_queries+=query_labels.size(0)\n",
    "\n",
    "        avg_acc_till=(total_correct/total_queries)*100\n",
    "        progress_bar.set_postfix(Loss=f\"{total_combined_loss.item():4f}\",Acc=f\"{avg_acc_till}\",PSNR=f\"{psnr}&\")\n",
    "\n",
    "\n",
    "    avg_recon_loss = total_recon_loss / len(few_dataloader)\n",
    "    avg_psnr = total_psnr / len(few_dataloader)\n",
    "    avg_loss=total_loss/len(few_dataloader)\n",
    "    accuracy=(total_correct/total_queries)*100\n",
    "    accuracy_spn_0=(total_correct/total_queries)*100\n",
    "    print(ways,\"way\",shots,\"shot:\",\"Loss=\",avg_loss,\"Acccuracy of Stable Prototypical Network on all\", len(test_list),\"Class =\",accuracy_spn_0,\"%\",\"PSNR=\",avg_psnr)\n",
    "\n",
    "\n",
    "    entropy = -(torch.softmax(logits, dim=1) * torch.log_softmax(logits, dim=1)).sum(dim=1).mean()\n",
    "    print(\"Mean Predictive Entropy =\", entropy.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
